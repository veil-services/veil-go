## **‚òÅÔ∏è Spec 005: Cloud Ingestion & Authentication**

**Status:** üü° Draft **Componente:** Veil Cloud (Backend API) **Foco:** Alta performance, Autentica√ß√£o de SDKs e Garantia de Privacidade.

---

### **1\. Vis√£o Geral**

Este componente √© a porta de entrada de dados. Ele deve ser capaz de receber milhares de eventos por segundo (fire-and-forget) dos SDKs rodando nos clientes. A lat√™ncia de ingest√£o n√£o afeta a lat√™ncia do cliente (pois √© ass√≠ncrono), mas afeta a atualiza√ß√£o do Dashboard.

### **2\. Autentica√ß√£o (API Keys)**

O modelo de auth segue o padr√£o Stripe/Resend.

* **Formato:** `veil_[env]_[random_string]`  
  * Ex: `veil_live_a1b2c3d4...` (Produ√ß√£o)  
  * Ex: `veil_test_x9y8z7...` (Ambiente de Teste/Sandbox)  
* **Header:** `Authorization: Bearer veil_live_...`  
* **Valida√ß√£o:** As chaves devem ser armazenadas com hash (para n√£o vazarem se o banco vazar) ou em um servi√ßo de segredos. Para v1, hash no banco (Postgres) com cache agressivo (Redis) para valida√ß√£o r√°pida.

### **3\. API de Ingest√£o (`POST /v1/events`)**

O SDK envia lotes (batches) de eventos para economizar conex√µes HTTP.

**Payload (JSON):**

JSON

```
{
  "sdk_version": "go/1.0.0",
  "app_name": "checkout-service", // Configurado no SDK
  "events": [
    {
      "timestamp": "2025-11-28T10:00:00Z",
      "endpoint": "/api/purchase", // Opcional, capturado via middleware
      "operation": "mask", // ou "restore"
      "duration_ms": 2,
      "pii_detected": {
        "cpf": 1,
        "email": 1,
        "credit_card": 0
      }
    }
  ]
}
```

**Restri√ß√£o de Privacidade (Hard Requirement):**

* O endpoint deve **rejeitar** (400 Bad Request) qualquer payload que contenha campos n√£o mapeados na whitelist acima.  
* **NUNCA** aceitar strings arbitr√°rias que possam conter o prompt do usu√°rio.

### **4\. Arquitetura de Ingest√£o (Async)**

Para n√£o derrubar o banco de dados com writes diretos:

1. **API Gateway:** Recebe o POST, valida Auth Key.  
2. **Message Queue:** Joga o payload em uma fila (RabbitMQ, SQS ou at√© Redis Streams/Kafka na v1).  
3. **Worker:** Consome a fila, agrega os dados (ex: incrementa contadores no DB) e salva para analytics.  
   * *Stack Sugerida v1:* Go \+ Redis Streams \+ Postgres (TimescaleDB ou apenas tabelas particionadas por data).

